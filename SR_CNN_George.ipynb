{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import All_RUT_Models\n",
    "import RUT_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "max_features = 153958\n",
    "\n",
    "modelname = 'CNN_George_w2v_sg_biased'\n",
    "\n",
    "modelpath = './Models/' + modelname + '/'\n",
    "\n",
    "if not os.path.exists( modelpath ):\n",
    "    os.makedirs( modelpath )\n",
    "if not os.path.exists( './Results/' ):\n",
    "    os.makedirs( './Results/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95692, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_sc(x):\n",
    "    alphanumeric = [character for character in x if (character.isalnum()) | (character==' ')]\n",
    "    alphanumeric = \"\".join(alphanumeric)\n",
    "    return alphanumeric\n",
    "\n",
    "df = pd.read_csv('wiki_train.csv')\n",
    "# df = pd.read_csv(r'training_data/wiki_train.csv')\n",
    "df = df[['comment', 'is_toxic']]\n",
    "df.columns = ['Comment', 'Toxic']\n",
    "df.Comment = df.Comment.astype( 'str' )\n",
    "df.Comment = df.Comment.apply(lambda x: x.lower())\n",
    "df.Comment = df.Comment.apply(lambda x: remove_sc(x))\n",
    "df['Toxic'][df['Toxic'].astype(bool) == True] = 1\n",
    "df['Toxic'][df['Toxic'].astype(bool) == False] = 0\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split( df.Comment, df.Toxic, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# clearing previous sessions\n",
    "K.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tokenization with keras tokenizer\n",
    "tokenizer = text.Tokenizer( num_words=max_features )\n",
    "tokenizer.fit_on_texts( X_train.values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "loaded_model = load_model( modelpath + modelname + '.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Jew</th>\n",
       "      <th>Muslim</th>\n",
       "      <th>Christian</th>\n",
       "      <th>Hindu</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>White</th>\n",
       "      <th>Female</th>\n",
       "      <th>...</th>\n",
       "      <th>@0.6</th>\n",
       "      <th>@0.65</th>\n",
       "      <th>@0.7</th>\n",
       "      <th>@0.75</th>\n",
       "      <th>@0.8</th>\n",
       "      <th>@0.85</th>\n",
       "      <th>@0.9</th>\n",
       "      <th>@0.95</th>\n",
       "      <th>Predicted_Probability_Nontoxic</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>murder american</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.977472</td>\n",
       "      <td>0.977472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respect trans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.991504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>respect homosexual</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.991504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hug mexican</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.717694</td>\n",
       "      <td>0.717694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kill sikh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.797720</td>\n",
       "      <td>0.202280</td>\n",
       "      <td>0.797720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comment  Toxic  Jew  Muslim  Christian  Hindu  Asian  Black  \\\n",
       "0     murder american      1    0       0          0      0      0      0   \n",
       "1       respect trans      0    0       0          0      0      0      0   \n",
       "2  respect homosexual      0    0       0          0      0      0      0   \n",
       "3         hug mexican      0    0       0          0      0      0      0   \n",
       "4           kill sikh      1    0       0          0      0      0      0   \n",
       "\n",
       "   White  Female  ...      @0.6     @0.65      @0.7     @0.75      @0.8  \\\n",
       "0      0       0  ...  0.022528  0.022528  0.022528  0.022528  0.022528   \n",
       "1      0       0  ...  0.008496  0.008496  0.008496  0.008496  0.008496   \n",
       "2      0       0  ...  0.008496  0.008496  0.008496  0.008496  0.008496   \n",
       "3      0       0  ...  0.282306  0.282306  0.282306  0.282306  0.282306   \n",
       "4      0       0  ...  0.797720  0.797720  0.797720  0.797720  0.797720   \n",
       "\n",
       "      @0.85      @0.9     @0.95  Predicted_Probability_Nontoxic   Maximum  \n",
       "0  0.022528  0.022528  0.022528                        0.977472  0.977472  \n",
       "1  0.008496  0.008496  0.008496                        0.991504  0.991504  \n",
       "2  0.008496  0.008496  0.008496                        0.991504  0.991504  \n",
       "3  0.282306  0.282306  0.282306                        0.717694  0.717694  \n",
       "4  0.797720  0.797720  0.797720                        0.202280  0.797720  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SR_Results/CNN_George_w2v_sg_biased_synthetic_predictions.csv')\n",
    "df['Predicted_Probability_Nontoxic'] = 1 - df.Predicted_Probability\n",
    "df['Maximum'] = df[['Predicted_Probability_Nontoxic', 'Predicted_Probability']].max(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Jew</th>\n",
       "      <th>Muslim</th>\n",
       "      <th>Christian</th>\n",
       "      <th>Hindu</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>White</th>\n",
       "      <th>Female</th>\n",
       "      <th>...</th>\n",
       "      <th>@0.6</th>\n",
       "      <th>@0.65</th>\n",
       "      <th>@0.7</th>\n",
       "      <th>@0.75</th>\n",
       "      <th>@0.8</th>\n",
       "      <th>@0.85</th>\n",
       "      <th>@0.9</th>\n",
       "      <th>@0.95</th>\n",
       "      <th>Predicted_Probability_Nontoxic</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>murder american</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977472</td>\n",
       "      <td>0.977472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respect trans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.991504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>respect homosexual</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.991504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hug mexican</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717694</td>\n",
       "      <td>0.717694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kill sikh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202280</td>\n",
       "      <td>0.797720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comment  Toxic  Jew  Muslim  Christian  Hindu  Asian  Black  \\\n",
       "0     murder american      1    0       0          0      0      0      0   \n",
       "1       respect trans      0    0       0          0      0      0      0   \n",
       "2  respect homosexual      0    0       0          0      0      0      0   \n",
       "3         hug mexican      0    0       0          0      0      0      0   \n",
       "4           kill sikh      1    0       0          0      0      0      0   \n",
       "\n",
       "   White  Female  ...  @0.6  @0.65  @0.7  @0.75  @0.8  @0.85  @0.9  @0.95  \\\n",
       "0      0       0  ...     0      0     0      0     0      0     0      0   \n",
       "1      0       0  ...     0      0     0      0     0      0     0      0   \n",
       "2      0       0  ...     0      0     0      0     0      0     0      0   \n",
       "3      0       0  ...     0      0     0      0     0      0     0      0   \n",
       "4      0       0  ...     1      1     1      1     1      1     1      1   \n",
       "\n",
       "   Predicted_Probability_Nontoxic   Maximum  \n",
       "0                        0.977472  0.977472  \n",
       "1                        0.991504  0.991504  \n",
       "2                        0.991504  0.991504  \n",
       "3                        0.717694  0.717694  \n",
       "4                        0.202280  0.797720  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = [0.55, 0.6, 0.65, 0.7, 0.75 , 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "for i in theta:\n",
    "    val = '@'+str(i)\n",
    "    df[val] = ( df[val] >= 0.5 ).astype( 'int32' )\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jew = ['jew', 'jewish', 'jewboy', 'zionis', 'torah']\n",
    "muslim = ['muslim', 'moslem', 'islam', 'mosque', 'hadith', 'mevlana', 'quran', 'koran', 'deobandi', 'shia', 'sunni', 'mohammed', 'mohemmed', 'mohammad', 'allah', 'jihadi', 'islamis']\n",
    "christian = ['christian', 'catholic', 'jesus', 'christ', 'cathedral', 'church', 'bible', 'priest', 'orthodox', 'baptis', 'pope']\n",
    "asian = ['asian', 'asian people', 'asians']\n",
    "black = ['black', 'blacks', 'black people', 'negro', 'nigga', 'nigger', 'nig', 'african', 'coon']\n",
    "white = ['white', 'whites', 'white people', 'caucasian', 'caucasoid', 'supermacist']\n",
    "female = ['aunt', 'her', 'mother', 'sister',  'sis', 'her', 'she', 'female', 'women', 'woman', 'girl', 'lady']\n",
    "male = ['uncle', ' his', 'father', 'bro', 'brother', 'him', ' he', ' male ', ' men ', ' man ', 'boy', 'guy', 'dude']\n",
    "homosexual = ['gay', 'lesbian', 'homosexual', 'homos']\n",
    "heterosexual = ['heterosexual', 'husband', 'wife']\n",
    "\n",
    "religious = {'jew': jew, 'muslim': muslim, 'christian': christian}\n",
    "race = {'asian': asian, 'black': black, 'white': white}\n",
    "gender = {'female': female, 'men': male}\n",
    "sexuality = {'homosexual': homosexual, 'heterosexual':heterosexual}\n",
    "             \n",
    "bias = {'religious': religious, 'race': race, 'gender': gender, 'sexuality': sexuality}\n",
    "\n",
    "\n",
    "def construct_favored_nn(comment, word, wordslist_id):\n",
    "    neighbors = []\n",
    "    for biastype_id in bias:\n",
    "        for wordslists in bias[biastype_id]:\n",
    "            if (wordslists not in wordslist_id):\n",
    "                for i in bias[biastype_id][wordslists]:\n",
    "                    neighbors.append(comment.replace(word, i))\n",
    "    neighbors.append(comment.replace(word, ''))\n",
    "    return neighbors\n",
    "\n",
    "def construct_deprived_nn(comment, wordslist_id):\n",
    "    neighbors = []\n",
    "    for biastype_id in bias:\n",
    "        for wordslists in bias[biastype_id]:\n",
    "            if (wordslists not in wordslist_id):\n",
    "                for i in bias[biastype_id][wordslists]:\n",
    "                    neighbors.append(comment + ' ' + i)\n",
    "    neighbors.append(comment)\n",
    "    return neighbors\n",
    "\n",
    "def situational_testing(comment):\n",
    "    #test for the group\n",
    "    found = False\n",
    "    word = ''\n",
    "    biastype_id = ''\n",
    "    wordslist_id = ''\n",
    "    nearest_neighbors = []\n",
    "    for bias_type in bias:\n",
    "        for wordslist in bias[bias_type]:\n",
    "            for term in bias[bias_type][wordslist]:\n",
    "                if ((term in comment) & (not found)):\n",
    "                    found = True\n",
    "                    word = term\n",
    "                    biastype_id = bias_type\n",
    "                    wordslist_id = wordslist\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    \n",
    "    if(biastype_id == ''):\n",
    "        nearest_neighbors = construct_deprived_nn(comment, wordslist_id)\n",
    "    else:     \n",
    "        nearest_neighbors = construct_favored_nn(comment, word, wordslist_id)\n",
    "        \n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def situational_roc(df, theta_val):\n",
    "    temp = df[df['Maximum'] < theta_val][['Sum', '@'+str(theta_val)]].index\n",
    "    for i in temp:\n",
    "        sentences = situational_testing(df.loc[i, 'Comment'])\n",
    "        comments = tokenizer.texts_to_sequences( sentences )\n",
    "        comments = sequence.pad_sequences( comments, maxlen=max_len )\n",
    "        predictions = loaded_model.predict( comments, verbose=0, batch_size=2048 )[ :, 1 ]\n",
    "        labels = (predictions>=0.5).astype( 'int32' ).sum() / len(predictions)\n",
    "        df.loc[i, '@'+str(theta_val)] = int(labels>=0.5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [22:38<00:00, 150.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 34s, sys: 1min 39s, total: 25min 14s\n",
      "Wall time: 22min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tqdm(theta):\n",
    "    df = situational_roc(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('SR_Results/CNN_George_w2v_sg_biased_synthetic_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
