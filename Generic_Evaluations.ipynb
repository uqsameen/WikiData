{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a method to measure termwise FNRs and FPRs along with the difference with Overall FNR and FPR\n",
    "wordslists = ['Jew', 'Muslim', 'Christian', 'Asian', 'Black', 'White', 'Female', 'Male', 'Heterosexual', 'Homosexual_gay_or_lesbian']\n",
    "scoresBeforeROC = {}\n",
    "scoresAfterROC = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Model(  ):\n",
    "    model = LogisticRegression( )\n",
    "    return model\n",
    "\n",
    "def ROC(df, theta):\n",
    "    df['PredictedLabel'][(df['Maximum'] < theta) & (df['Sum'] == 0)] = 1 #deprived\n",
    "    df['PredictedLabel'][(df['Maximum'] < theta) & (df['Sum'] >= 1)] = 0 #favored\n",
    "    return df\n",
    "\n",
    "def ROC_BAT(df, theta, bat):\n",
    "    #ROC\n",
    "    df['PredictedLabel'][(df['Maximum'] < theta) & (df['Sum'] == 0)] = 1 #deprived\n",
    "    df['PredictedLabel'][(df['Maximum'] < theta) & (df['Sum'] >= 1)] = 0 #favored\n",
    "    #BAT\n",
    "    df['PredictedLabel'][(df['Sum'] == 0) & (df['Difference'] > bat)] = 1\n",
    "    df['PredictedLabel'][(df['Sum'] == 0) & (df['Difference'] < bat)] = 0\n",
    "    df['PredictedLabel'][(df['Sum'] >= 1) & (df['Difference'] > bat)] = 1\n",
    "    df['PredictedLabel'][(df['Sum'] >= 1) & (df['Difference'] < bat)] = 0\n",
    "    return df\n",
    "\n",
    "def evaluations(df):\n",
    "    df.TrueLabel = df.TrueLabel.astype('int')\n",
    "    df.PredictedLabel = df.PredictedLabel.astype('int')\n",
    "    accuracy = accuracy_score( df.TrueLabel, df.PredictedLabel ) \n",
    "    precision = precision_score( df.TrueLabel, df.PredictedLabel ) \n",
    "    recall = recall_score( df.TrueLabel, df.PredictedLabel ) \n",
    "    f1 = f1_score( df.TrueLabel, df.PredictedLabel ) \n",
    "    cr = classification_report( df.TrueLabel, df.PredictedLabel )\n",
    "    cm = confusion_matrix( df.TrueLabel, df.PredictedLabel )\n",
    "    \n",
    "    _tn, _fp, _fn, _tp = cm.ravel()\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cr, cm, _tn, _fp, _fn, _tp \n",
    "\n",
    "#FNRs, FPRs, Equalized Error Rates\n",
    "def EERs(df, wordslists, scores, tnoverall, fpoverall, fnoverall, tpoverall):\n",
    "    fnrd, fprd = 0, 0\n",
    "    for i in wordslists:\n",
    "        temp = df[df[i] == 1][['Comment', 'ProbabilityNontoxic', 'ProbabilityToxic', 'TrueLabel', 'PredictedLabel', 'Maximum']]\n",
    "        if(len(temp) > 0):\n",
    "            cm = confusion_matrix( temp.TrueLabel, temp.PredictedLabel )\n",
    "            fp, fn = cm.ravel()[1], cm.ravel()[2]\n",
    "            scores[i] = {}\n",
    "            scores[i]['fp'] = fp\n",
    "            scores[i]['fn'] = fn\n",
    "        fprd = fprd + (fpoverall - fp)\n",
    "        fnrd = fnrd + (fnoverall - fn)\n",
    "    pbr = (fpoverall - fnoverall) / (tnoverall + fpoverall + fnoverall + tpoverall)\n",
    "    return scores, fnrd, fprd, pbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f6ac30293a01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'training_data/wiki_debias_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# df = pd.read_csv(r'training_data/wiki_train.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is_toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#training set\n",
    "df = pd.read_csv(r'training_data/wiki_debias_train.csv')\n",
    "# df = pd.read_csv(r'training_data/wiki_train.csv')\n",
    "df = df[['comment', 'is_toxic']]\n",
    "df.columns = ['Comment', 'Toxic']\n",
    "df['Toxic'][df['Toxic'].astype(bool) == True] = 1\n",
    "df['Toxic'][df['Toxic'].astype(bool) == False] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data into train and test\n",
    "X_train, X_test, ytrain, ytest  = train_test_split( df.Comment, df.Toxic, test_size = 0.10, random_state = 1990)\n",
    "\n",
    "#tokenize train and test then padd with given values\n",
    "vectorizer = TfidfVectorizer(  )\n",
    "vectorizer.fit( X_train )\n",
    "\n",
    "xtrain = vectorizer.transform( X_train )\n",
    "xtest = vectorizer.transform( X_test )\n",
    "\n",
    "#define a model\n",
    "model = ML_Model(  )\n",
    "#train the model\n",
    "model.fit( xtrain, ytrain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_pipeline(vectorizer, model, X_test, ytest):\n",
    "    c = make_pipeline(vectorizer, model)\n",
    "\n",
    "    class_names = ['Nontoxic', 'Toxic']\n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "    predictions_df = pd.DataFrame()\n",
    "    predictions_df['Comment'] = X_test\n",
    "    probabilities = c.predict_proba(list(X_test))\n",
    "    predictions_df['ProbabilityNontoxic'] = probabilities[:, 0]\n",
    "    predictions_df['ProbabilityToxic'] = probabilities[:, 1]\n",
    "    #predictions_df['PredictedValue'] = c.predict_proba(list(X_test))[:, 1]\n",
    "    predictions_df['TrueLabel'] = ytest\n",
    "\n",
    "    predictions_df['PredictedLabel'] = 0\n",
    "    predictions_df['PredictedLabel'][(predictions_df['ProbabilityToxic'] >= 0.5)] = 1\n",
    "\n",
    "    predictions_df['Maximum'] = predictions_df[['ProbabilityToxic', 'ProbabilityNontoxic']].max(axis=1)\n",
    "    predictions_df['Difference'] = abs(predictions_df['ProbabilityToxic'] - predictions_df['ProbabilityNontoxic'])\n",
    "\n",
    "    predictions_df.reset_index(drop=True, inplace=True)\n",
    "    predictions_df.head()\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534086325131101 0.8934707903780069 0.5652173913043478 0.6924101198402131\n",
      "8934 62 400 520\n"
     ]
    }
   ],
   "source": [
    "predictions_df = lime_pipeline(vectorizer, model, X_test, ytest)\n",
    "accuracy, precision, recall, f1, cr, cm, _tn, _fp, _fn, _tp = evaluations(predictions_df)\n",
    "print(accuracy, precision, recall, f1)\n",
    "print(_tn, _fp, _fn, _tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76564, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Jew</th>\n",
       "      <th>Muslim</th>\n",
       "      <th>Christian</th>\n",
       "      <th>Hindu</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>White</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Heterosexual</th>\n",
       "      <th>Homosexual_gay_or_lesbian</th>\n",
       "      <th>PredictedLabel</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>murder american</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respect trans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>respect homosexual</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hug mexican</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kill sikh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comment  Toxic  Jew  Muslim  Christian  Hindu  Asian  Black  \\\n",
       "0     murder american      1    0       0          0      0      0      0   \n",
       "1       respect trans      0    0       0          0      0      0      0   \n",
       "2  respect homosexual      0    0       0          0      0      0      0   \n",
       "3         hug mexican      0    0       0          0      0      0      0   \n",
       "4           kill sikh      1    0       0          0      0      0      0   \n",
       "\n",
       "   White  Female  Male  Heterosexual  Homosexual_gay_or_lesbian  \\\n",
       "0      0       0     0             0                          0   \n",
       "1      0       0     0             0                          0   \n",
       "2      0       0     0             0                          1   \n",
       "3      0       0     0             0                          0   \n",
       "4      0       0     0             0                          0   \n",
       "\n",
       "   PredictedLabel  Sum  \n",
       "0             0.0    0  \n",
       "1             0.0    0  \n",
       "2             1.0    1  \n",
       "3             0.0    0  \n",
       "4             0.0    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv(r'testing_data/Synthetic_debias_test.csv')\n",
    "# testset = testset [ testset['Sum'] > 0 ]\n",
    "# testset.reset_index(drop=True, inplace=True)\n",
    "print (testset.shape)\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6038347003813803 0.9963786213786214 0.2084269369416436 0.3447396845971052\n",
      "38253 29 30303 7979\n"
     ]
    }
   ],
   "source": [
    "#Before ROC\n",
    "predictions_df = lime_pipeline(vectorizer, model, testset.Comment, testset.Toxic)\n",
    "output_df = testset.join(predictions_df, lsuffix='test')\n",
    "output_df.drop(columns=['Commenttest', 'PredictedLabeltest'], inplace=True)\n",
    "accuracy, precision, recall, f1, cr, cm, _tn, _fp, _fn, _tp = evaluations(output_df)\n",
    "print(accuracy, precision, recall, f1)\n",
    "print(_tn, _fp, _fn, _tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322935 294 -0.3954077634397367\n"
     ]
    }
   ],
   "source": [
    "scoresb4, fnrdb4, fprdb4, pbrb4 = EERs(output_df, wordslists, scoresBeforeROC, tnoverall=_tn, fpoverall=_fp, fnoverall=_fn, tpoverall=_tp)\n",
    "print(fnrdb4, fprdb4, pbrb4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149887675670027 0.9959441189725101 0.23091792487330862 0.37490987743330934\n",
      "38246 36 29442 8840\n"
     ]
    }
   ],
   "source": [
    "#After ROC\n",
    "roc_output_df = ROC(output_df, 0.6)\n",
    "accuracy, precision, recall, f1, cr, cm, _tn, _fp, _fn, _tp = evaluations(roc_output_df)\n",
    "print(accuracy, precision, recall, f1)\n",
    "print(_tn, _fp, _fn, _tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312547 386 -0.3840708426936942\n"
     ]
    }
   ],
   "source": [
    "scoresAfter, fnrdAfter, fprdAfter, pbrAfter = EERs(roc_output_df, wordslists, scoresAfterROC, tnoverall=_tn, fpoverall=_fp, fnoverall=_fn, tpoverall=_tp)\n",
    "print(fnrdAfter, fprdAfter, pbrAfter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
